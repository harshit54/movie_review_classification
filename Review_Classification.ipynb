{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Review_Classification",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshit54/movie_review_classification/blob/main/Review_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4espuLKJiA"
      },
      "source": [
        "## This notebook is licensed with The Unlicense\n",
        "For more information, please refer [here](https://unlicense.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ6SNYq_tVVC"
      },
      "source": [
        "# Classifying text with BERT\n",
        "\n",
        "In this notebook, I have:\n",
        "\n",
        "- Loaded the IMDB dataset\n",
        "- Loaded a BERT model from TensorFlow Hub\n",
        "- Built the model by combining BERT with a Classifier\n",
        "- Training the model for fine-tuning BERT\n",
        "- Analyzing the testing process on TensorBoard\n",
        "- Saving the model and classifying some reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjmX4zTCkRK"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-YbjCkzw0yU"
      },
      "source": [
        "# Installing TensorFlow Text Library for using in Preprocessing\n",
        "!pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_XlxN1IsRJ"
      },
      "source": [
        "Importing the AdamW optimizer from `tensorflow/models`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-P1ZOA0FkVJ"
      },
      "source": [
        "!pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6MugfEgDRpY"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "The model will classify the reviews as *positive* or *negative*, based on the text of the review.\n",
        "\n",
        "I have used the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnvd4mrtPHHV"
      },
      "source": [
        "### Downloading the IMDB dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOdqCMoQDRJL"
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3harCfQGN6M"
      },
      "source": [
        "## Importing the Data as a TensorFlow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IwI_2bcIeX8"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',q\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoHydxMtGWqS"
      },
      "source": [
        "## Selecting the ```small_bert``` Classifier from Tensorflow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_ctG55-uTX"
      },
      "source": [
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WrcxxTRDdHi"
      },
      "source": [
        "## Importing the Respective Preprocessing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SQi-jWd_jzq"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDNKfAXbDnJH"
      },
      "source": [
        "## Defining The Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aksj743St9ga"
      },
      "source": [
        "def build_classifier_model():\n",
        "  tf.config.run_functions_eagerly(False)\n",
        "\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "\n",
        "  # Single Neuron as Ouput Layer\n",
        "  \n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v32MEE_JFuQh"
      },
      "source": [
        "## Model Diagram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EmzyHZXKIpm"
      },
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbUWoZMwc302"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpJ3xcwDT56v"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "Since this is a binary classification problem and the model outputs a probability (a single-unit layer),`losses.BinaryCrossentropy` loss function will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWPOZE-L3AgE"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77psrpfzbxtp"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqlarlpC_v0g"
      },
      "source": [
        "### Loading the BERT model and training\n",
        "\n",
        "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7GPDhR98jsD"
      },
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDv5Cud3IraH"
      },
      "source": [
        "## Plotting on TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGQYI6qlIFpE"
      },
      "source": [
        "### Setting Up Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vRDgzOgDoTb"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ28z41QG20z"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "log_dir=\"./logs\"\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__YwpzHuIQaY"
      },
      "source": [
        "## Training The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtfDFAnN_Neu"
      },
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data=val_ds,\n",
        "                               epochs=epochs,\n",
        "                               callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bVt5RC3H7Se"
      },
      "source": [
        "### Opening Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVvMAdFDHQIC"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBthMlTSV8kn"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slqB-urBV9sP"
      },
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyTappHTvNCz"
      },
      "source": [
        "## Testing The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBWzH6exlCPS"
      },
      "source": [
        "def print_my_examples(inputs, results):\n",
        "  result_for_printing = \\\n",
        "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
        "                         for i in range(len(inputs))]\n",
        "  print(*result_for_printing, sep='\\n')\n",
        "  print()\n",
        "\n",
        "\n",
        "examples = [\n",
        "    'this is such an amazing movie!',\n",
        "    'Best Movie Ever!',\n",
        "    'Totally worth the wait.',\n",
        "    'The movie was okish.',\n",
        "    'The movie was a nightmare'\n",
        "]\n",
        "\n",
        "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
        "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
        "\n",
        "print('Results from the saved model:')\n",
        "print_my_examples(examples, reloaded_results)\n",
        "print('Results from the model in memory:')\n",
        "print_my_examples(examples, original_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}